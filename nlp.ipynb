{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are Corpus, Tokens, and Engrams?\n",
    "A Corpus is defined as a collection of text documents for example a Twitter data set containing the tweets. Twitter data is a corpus. So corpus consists of documents, documents comprise paragraphs, paragraphs comprise sentences and sentences comprise further smaller units which are called Tokens.\n",
    "Tokens can be words, phrases, or Engrams, and Engrams are defined as the group of n words together.\n",
    "Refer this link <https://www.analyticsvidhya.com/blog/2021/02/basics-of-natural-language-processing-nlp-basics/> for more details.\n",
    "\n",
    "Structure of a token :  prefix + morphene + suffix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Preprocessing\n",
    "\n",
    "1. Noise Removal - It involves removing stop words (is, am, the), social media entities (mentions, hashtags), punctuations.\n",
    "                    In the process tweet function, we can see it where using regex we are removing them. For stopwords, we are using nltk library.\n",
    "\n",
    "2. Normalization -  a. Stemming: Simply removing suffixes from a word (Not recommended because it may give words which are not in vocabulary)\n",
    "                    b. Lemmatization: Lemmatization, on the other hand, is an organized & step by step procedure of obtaining the root form of the word, it makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations).\n",
    "\n",
    "3. Object Standardization - It involves handling slangs, lingos or non-standard words with words from vocabulary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trade off between Stemming and Lemmatization:\n",
    "The choice depends on the specific use case. Lemmatization produces a linguistically valid word while stemming is faster but may generate non-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise Removal \n",
    "\n",
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# With Stemming\n",
    "def process_tweet(tweet):\n",
    "    \"\"\"Process tweet function.\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks    \n",
    "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # Lowercase the words\n",
    "    tweet = tweet.lower()\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\MOHAMMED\n",
      "[nltk_data]     USAMA\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download it if you have not download the wordnet\n",
    "#import nltk\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed: multipli\n",
      "Lemmatized as verb: multiply\n"
     ]
    }
   ],
   "source": [
    "# Normalization\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "stem = PorterStemmer()\n",
    "\n",
    "word = \"multiplying\"\n",
    "\n",
    "# Apply stemming\n",
    "print(\"Stemmed:\", stem.stem(word))\n",
    "\n",
    "# Apply lemmatization, specifying that the word is a verb\n",
    "print(\"Lemmatized as verb:\", lem.lemmatize(word, 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Lemmatization\n",
    "def process_tweet_Lem(tweet):\n",
    "    \"\"\"Process tweet function.\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "\n",
    "    \"\"\"\n",
    "    lem = WordNetLemmatizer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks    \n",
    "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # Lowercase the words\n",
    "    tweet = tweet.lower()\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            lemm_text = lem.lemmatize(word)  # Lemmatizing word\n",
    "            tweets_clean.append(lemm_text)\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Founded in 2002, SpaceX‚Äôs mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
    "species by building a self-sustaining city on Mars. In 2008, SpaceX‚Äôs Falcon 1 became the first privately developed \n",
    "liquid-fuel launch vehicle to orbit the Earth.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['found',\n",
       " '2002',\n",
       " 'spacex',\n",
       " '‚Äô',\n",
       " 'mission',\n",
       " 'enabl',\n",
       " 'human',\n",
       " 'becom',\n",
       " 'spacefar',\n",
       " 'civil',\n",
       " 'multi-planet',\n",
       " 'speci',\n",
       " 'build',\n",
       " 'self-sustain',\n",
       " 'citi',\n",
       " 'mar',\n",
       " '2008',\n",
       " 'spacex',\n",
       " '‚Äô',\n",
       " 'falcon',\n",
       " '1',\n",
       " 'becam',\n",
       " 'first',\n",
       " 'privat',\n",
       " 'develop',\n",
       " 'liquid-fuel',\n",
       " 'launch',\n",
       " 'vehicl',\n",
       " 'orbit',\n",
       " 'earth']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing and Stemming\n",
    "process_tweet(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['founded',\n",
       " '2002',\n",
       " 'spacex',\n",
       " '‚Äô',\n",
       " 'mission',\n",
       " 'enable',\n",
       " 'human',\n",
       " 'become',\n",
       " 'spacefaring',\n",
       " 'civilization',\n",
       " 'multi-planet',\n",
       " 'specie',\n",
       " 'building',\n",
       " 'self-sustaining',\n",
       " 'city',\n",
       " 'mar',\n",
       " '2008',\n",
       " 'spacex',\n",
       " '‚Äô',\n",
       " 'falcon',\n",
       " '1',\n",
       " 'became',\n",
       " 'first',\n",
       " 'privately',\n",
       " 'developed',\n",
       " 'liquid-fuel',\n",
       " 'launch',\n",
       " 'vehicle',\n",
       " 'orbit',\n",
       " 'earth']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing and Lemmatizing\n",
    "process_tweet_Lem(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization using different librarires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Founded', 'in', '2002', ',', 'SpaceX', '‚Äô', 's', 'mission', 'is', 'to', 'enable', 'humans', 'to', 'become', 'a', 'spacefaring', 'civilization', 'and', 'a', 'multi-planet', 'species', 'by', 'building', 'a', 'self-sustaining', 'city', 'on', 'Mars', '.', 'In', '2008', ',', 'SpaceX', '‚Äô', 's', 'Falcon', '1', 'became', 'the', 'first', 'privately', 'developed', 'liquid-fuel', 'launch', 'vehicle', 'to', 'orbit', 'the', 'Earth', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Founded in 2002, SpaceX‚Äôs mission is to enable humans to become a spacefaring civilization and a multi-planet species by building a self-sustaining city on Mars.',\n",
       " 'In 2008, SpaceX‚Äôs Falcon 1 became the first privately developed liquid-fuel launch vehicle to orbit the Earth.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk\n",
    "# Before running using this code\n",
    "#nltk.download('punkt') \n",
    "\n",
    "# word tokenize\n",
    "from nltk.tokenize import word_tokenize \n",
    "text = \"\"\"Founded in 2002, SpaceX‚Äôs mission is to enable humans to become a spacefaring civilization and a multi-planet species by building a self-sustaining city on Mars. In 2008, SpaceX‚Äôs Falcon 1 became the first privately developed liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
    "\n",
    "print(word_tokenize(text))\n",
    "\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\MOHAMMED USAMA\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['founded',\n",
       " 'in',\n",
       " '2002',\n",
       " 'spacex‚Äôs',\n",
       " 'mission',\n",
       " 'is',\n",
       " 'to',\n",
       " 'enable',\n",
       " 'humans',\n",
       " 'to',\n",
       " 'become',\n",
       " 'a',\n",
       " 'spacefaring',\n",
       " 'civilization',\n",
       " 'and',\n",
       " 'a',\n",
       " 'multi',\n",
       " 'planet',\n",
       " 'species',\n",
       " 'by',\n",
       " 'building',\n",
       " 'a',\n",
       " 'self',\n",
       " 'sustaining',\n",
       " 'city',\n",
       " 'on',\n",
       " 'mars',\n",
       " 'in',\n",
       " '2008',\n",
       " 'spacex‚Äôs',\n",
       " 'falcon',\n",
       " '1',\n",
       " 'became',\n",
       " 'the',\n",
       " 'first',\n",
       " 'privately',\n",
       " 'developed',\n",
       " 'liquid',\n",
       " 'fuel',\n",
       " 'launch',\n",
       " 'vehicle',\n",
       " 'to',\n",
       " 'orbit',\n",
       " 'the',\n",
       " 'earth']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using keras\n",
    "\n",
    "# Word Tokenization\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# define\n",
    "text = \"\"\"Founded in 2002, SpaceX‚Äôs mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
    "species by building a self-sustaining city on Mars. In 2008, SpaceX‚Äôs Falcon 1 became the first privately developed \n",
    "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
    "# tokenize\n",
    "result = text_to_word_sequence(text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Founded',\n",
       " 'in',\n",
       " 'SpaceX',\n",
       " 's',\n",
       " 'mission',\n",
       " 'is',\n",
       " 'to',\n",
       " 'enable',\n",
       " 'humans',\n",
       " 'to',\n",
       " 'become',\n",
       " 'a',\n",
       " 'spacefaring',\n",
       " 'civilization',\n",
       " 'and',\n",
       " 'a',\n",
       " 'multi',\n",
       " 'planet',\n",
       " 'species',\n",
       " 'by',\n",
       " 'building',\n",
       " 'a',\n",
       " 'self',\n",
       " 'sustaining',\n",
       " 'city',\n",
       " 'on',\n",
       " 'Mars',\n",
       " 'In',\n",
       " 'SpaceX',\n",
       " 's',\n",
       " 'Falcon',\n",
       " 'became',\n",
       " 'the',\n",
       " 'first',\n",
       " 'privately',\n",
       " 'developed',\n",
       " 'liquid',\n",
       " 'fuel',\n",
       " 'launch',\n",
       " 'vehicle',\n",
       " 'to',\n",
       " 'orbit',\n",
       " 'the',\n",
       " 'Earth']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using gensim\n",
    "#!pip install gensim==3.8.3\n",
    "\n",
    "# word tokenization\n",
    "from gensim.utils import tokenize\n",
    "text = \"\"\"Founded in 2002, SpaceX‚Äôs mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
    "species by building a self-sustaining city on Mars. In 2008, SpaceX‚Äôs Falcon 1 became the first privately developed \n",
    "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
    "list(tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Regex Code required for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.leetcode.com/'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To find a URL in a sentence\n",
    "def find_url(string):\n",
    "    text = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*(),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',string)\n",
    "    #convert return value from list to string    \n",
    "    return \"\".join(text)\n",
    "\n",
    "example=\"I love spending time at https://www.leetcode.com/\"\n",
    "find_url(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['soccer_ball', 'beaming_face_with_smiling_eyes']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install emoji \n",
    "import emoji\n",
    "\n",
    "# To find emoticons in a sentence\n",
    "def findEmoji(text):\n",
    "    emo_text=emoji.demojize(text)\n",
    "    line=re.findall(r':(.*?):',emo_text)\n",
    "    return line\n",
    "\n",
    "example=\"I love ‚öΩ very much üòÅ\"\n",
    "findEmoji(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsgs111@gmail.com\n"
     ]
    }
   ],
   "source": [
    "# To find an email\n",
    "\n",
    "def findEmail(text):\n",
    "    # Improved regex pattern for matching email addresses\n",
    "    line = re.findall(r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+', str(text))\n",
    "    return \",\".join(line)\n",
    "\n",
    "# Example usage\n",
    "example = \"Gaurav's gmail is gsgs111@gmail.com\"\n",
    "print(findEmail(example))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
